# Substack Scraper Configuration
# Edit these settings to customize the scraper behavior

[scraper]
# Maximum number of profiles to scrape (0 = unlimited)
max_profiles = 0

# Number of concurrent profiles to scrape at once (faster but uses more resources)
concurrent_profiles = 100

# Number of times to scroll the explore page (more scrolls = more profiles loaded)
scroll_times = 0

# Maximum subscriber count to include (profiles above this will be skipped)
# Set to 0 to disable filtering
max_subscribers = 0

# Minimum subscriber count to include
# Set to 0 to disable filtering
min_subscribers = 1000

[browser]
# Run browser in headless mode (no visible window)
# Set to true for faster scraping, false for debugging
headless = true

# Page load timeout in milliseconds
timeout_ms = 30000

# Wait time after page load (milliseconds) - reduced for speed
page_wait_ms = 2000

# Wait time after each scroll (milliseconds) - reduced for speed
scroll_wait_ms = 2000

# Delay between each profile request (milliseconds) - reduced for speed
request_delay_ms = 1000

# Extra delay after errors (milliseconds)
error_delay_ms = 5000

# Number of retries for failed requests
max_retries = 3

[output]
# Output format: csv or json
format = csv

# Output filename (without extension, extension will be added based on format)
filename = substack_profiles

# Output directory (use "." for current directory)
output_dir = /Users/aadi/Documents/automated_testing

[filters]
# Comma-separated list of social platforms to extract
# Leave empty to extract all
# Options: twitter, instagram, tiktok, linkedin, facebook, youtube, github, threads, bluesky
platforms = 

# Only include profiles that have at least one social link
require_social_links = false


# Output file for YouTube emails
output_file = youtube_emails.csv

# Input file (the Substack profiles CSV)
input_file = substack_profiles.csv

# Delay between requests (milliseconds) - keep high to avoid detection
min_delay_ms = 4000
max_delay_ms = 10000

# Maximum retries per channel
max_retries = 2

# Run browser in headless mode (set to false to see the browser)
headless = false

# File to track processed URLs (for resume capability)
processed_file = youtube_processed.txt
